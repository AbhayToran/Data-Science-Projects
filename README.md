# ğŸ“Š Data Science Projects

This repository contains two end-to-end **data-to-model pipeline** projects demonstrating both **Classification** and **Regression** techniques using Python (Pandas, NumPy, Scikit-learn).

---

## ğŸ“‚ Repository Structure
Data_Science_Project/
â”‚â”€â”€ Subsidy_question.py
â”‚â”€â”€ Linear_Regression_cars.py
â”‚â”€â”€ README.md

---

## ğŸ­. ğ—–ğ—¹ğ—®ğ˜€ğ˜€ğ—¶ğ—³ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜: ğ—œğ—»ğ—°ğ—¼ğ—ºğ—²-ğ—•ğ—®ğ˜€ğ—²ğ—± ğ—¦ğ˜‚ğ—¯ğ˜€ğ—¶ğ—±ğ˜† ğ—£ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ğ—¶ğ—¼ğ—»  
**File:** `Subsidy_question.py`

This project addressed a **classification problem** to predict whether an individual's income is **â‰¤ $50,000** or **> $50,000**, which can be used for determining **subsidy eligibility**.

### ğ——ğ—®ğ˜ğ—® ğ—–ğ—¹ğ—²ğ—®ğ—»ğ—¶ğ—»ğ—´ & ğ—£ğ—¿ğ—²ğ—½ğ—¿ğ—¼ğ—°ğ—²ğ˜€ğ˜€ğ—¶ğ—»ğ—´
- **Initial Cleanup:** Dataset initially showed no null values, but placeholder â€œ?â€ values were found in `JobType` and `occupation`.  
- **Missing Value Handling:** Converted â€œ?â€ to NaN and dropped missing rows (~1,816 records) â†’ Final dataset: **30,162 records**.  
- **Feature Analysis:** Used `pd.crosstab` to analyze categorical features (`EdType`, `occupation`, `gender`) against the target `SalStat`.  
- **Encoding:** Mapped `SalStat` to binary (â‰¤$50,000 â†’ 0, >$50,000 â†’ 1), applied **one-hot encoding** â†’ dataset expanded to **94 input columns**.  

### ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—•ğ˜‚ğ—¶ğ—¹ğ—±ğ—¶ğ—»ğ—´ & ğ—˜ğ˜ƒğ—®ğ—¹ğ˜‚ğ—®ğ˜ğ—¶ğ—¼ğ—»
- Train/Test split: **70% train / 30% test**.  
- **Logistic Regression:** Accuracy = **83.61%**  
- **K-Nearest Neighbors (KNN, k=5):** Accuracy = **83.92%**

---

## ğŸ®. ğ—¥ğ—²ğ—´ğ—¿ğ—²ğ˜€ğ˜€ğ—¶ğ—¼ğ—» ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜: ğ—£ğ—¿ğ—²-ğ—¢ğ˜„ğ—»ğ—²ğ—± ğ—–ğ—®ğ—¿ ğ—£ğ—¿ğ—¶ğ—°ğ—² ğ—£ğ—¿ğ—²ğ—±ğ—¶ğ—°ğ˜ğ—¶ğ—¼ğ—»  
**File:** `Linear_Regression_cars.py`

This project used **Linear Regression** and **Random Forest Regressor** to predict the **continuous price** of pre-owned cars.

### ğ——ğ—®ğ˜ğ—® ğ—–ğ˜‚ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—» & ğ—™ğ—²ğ—®ğ˜ğ˜‚ğ—¿ğ—² ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´
- **Initial Data:** 50,001 records across 19 columns.  
- **Column Dropping:** Removed 5 irrelevant columns (`dateCrawled`, `dateCreated`, `postalCode`, etc.).  
- **Deduplication:** Removed 470 duplicate records.  
- **Outlier Treatment:**  
  - `yearOfRegistration`: 1980â€“2025  
  - `price`: $100â€“$150,000  
  - `powerPS`: 10â€“500 PS  
  â†’ Final dataset after cleaning: **42,417 records**.  
- **Feature Creation:** `Age = 2025 â€“ yearOfRegistration â€“ (monthOfRegistration / 12)`.  
- **Dropped Low-Impact Features:** `seller`, `offerType`, `abtest`.  
- **Final Preprocessing:** Removed remaining NaNs â†’ **32,765 records**. Converted categorical features to dummy variables â†’ **300 input columns**.  
- **Target Transformation:** Applied **logarithmic transformation** to `price`.  

### ğ— ğ—¼ğ—±ğ—²ğ—¹ ğ—§ğ—¿ğ—®ğ—¶ğ—»ğ—¶ğ—»ğ—´ & ğ—–ğ—¼ğ—ºğ—½ğ—®ğ—¿ğ—®ğ˜ğ—¶ğ˜ƒğ—² ğ—¥ğ—²ğ˜€ğ˜‚ğ—¹ğ˜ğ˜€
- Train/Test Split: 70:30 â†’ **22,872 train / 9,803 test**  
- **Baseline (Mean Prediction):** RMSE = 1.143  

**Models:**  
- **Linear Regression** â†’ RMSE: **0.4863**, RÂ²: **0.8191**  
- **Random Forest Regressor** â†’ RMSE: **0.4877**, RÂ²: **0.8181**  

### ğ—–ğ—¼ğ—»ğ—°ğ—¹ğ˜‚ğ˜€ğ—¶ğ—¼ğ—»
Linear Regression provided the best performance, explaining **~81.9% variance** in car price with lower RMSE.  

---

## ğŸ› ï¸ Tech Stack
- Python (Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn)  
- Jupyter/Spyder IDE  
- Data preprocessing, feature engineering, model training & evaluation  

---

## ğŸ“Œ Key Learnings
- Importance of **robust data cleaning** and handling missing values.  
- **Feature engineering** significantly improves model accuracy.  
- Target transformation (log scale) helps stabilize regression performance.  
- Comparative model analysis provides better decision-making.  

---
